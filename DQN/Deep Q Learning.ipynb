{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from gym.wrappers import Monitor\n",
    "import time\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\")\n",
    "\n",
    "from lib import plotting\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-16 18:26:25,860] Making new env: Breakout-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.envs.make(\"Breakout-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Atari Actions: 0 (noop), 1 (fire), 2 (left) and 3 (right) are valid actions\n",
    "VALID_ACTIONS = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StateProcessor():\n",
    "    \"\"\"\n",
    "    Processes a raw Atari iamges. Resizes it and converts it to grayscale.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Build the Tensorflow graph\n",
    "        with tf.variable_scope(\"state_processor\"):\n",
    "            self.input_state = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "            self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
    "            self.output = tf.image.crop_to_bounding_box(self.output, 34, 0, 160, 160)\n",
    "            self.output = tf.image.resize_images(\n",
    "                self.output, np.array([84, 84],dtype=np.int32), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            self.output = tf.squeeze(self.output)\n",
    "\n",
    "    def process(self, sess, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            state: A [210, 160, 3] Atari RGB State\n",
    "\n",
    "        Returns:\n",
    "            A processed [84, 84, 1] state representing grayscale values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.output, { self.input_state: state })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"Q-Value Estimator neural network.\n",
    "\n",
    "    This network is used for both the Q-Network and the Target Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scope=\"estimator\", summaries_dir=None):\n",
    "        self.scope = scope\n",
    "        # Writes Tensorboard summaries to disk\n",
    "        self.summary_writer = None\n",
    "        with tf.variable_scope(scope):\n",
    "            # Build the graph\n",
    "            self._build_model()\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the Tensorflow graph.\n",
    "        \"\"\"\n",
    "\n",
    "        # Placeholders for our input\n",
    "        # Our input are 4 RGB frames of shape 160, 160 each\n",
    "        self.X_pl = tf.placeholder(shape=[None, 84, 84, 4], dtype=tf.uint8, name=\"X\")\n",
    "        # The TD target value\n",
    "        self.y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "        # Integer id of which action was selected\n",
    "        self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "\n",
    "        X = tf.to_float(self.X_pl) / 255.0\n",
    "        batch_size = tf.shape(self.X_pl)[0]\n",
    "\n",
    "        # Three convolutional layers\n",
    "        conv1 = tf.contrib.layers.conv2d(\n",
    "            X, 32, 8, 4, activation_fn=tf.nn.relu)\n",
    "        conv2 = tf.contrib.layers.conv2d(\n",
    "            conv1, 64, 4, 2, activation_fn=tf.nn.relu)\n",
    "        conv3 = tf.contrib.layers.conv2d(\n",
    "            conv2, 64, 3, 1, activation_fn=tf.nn.relu)\n",
    "\n",
    "        # Fully connected layers\n",
    "        flattened = tf.contrib.layers.flatten(conv3)\n",
    "        fc1 = tf.contrib.layers.fully_connected(flattened, 512)\n",
    "        self.predictions = tf.contrib.layers.fully_connected(fc1, len(VALID_ACTIONS))\n",
    "\n",
    "        # Get the predictions for the chosen actions only\n",
    "        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl\n",
    "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "\n",
    "        # Calcualte the loss\n",
    "        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "\n",
    "        # Optimizer Parameters from original paper\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "\n",
    "        # Summaries for Tensorboard\n",
    "        self.summaries = tf.summary.merge([\n",
    "            tf.summary.scalar(\"loss\", self.loss),\n",
    "            tf.summary.histogram(\"loss_hist\", self.losses),\n",
    "            tf.summary.histogram(\"q_values_hist\", self.predictions),\n",
    "            tf.summary.scalar(\"max_q_value\", tf.reduce_max(self.predictions))\n",
    "        ])\n",
    "\n",
    "\n",
    "    def predict(self, sess, s):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session\n",
    "          s: State input of shape [batch_size, 4, 160, 160, 3]\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated \n",
    "          action values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.predictions, { self.X_pl: s })\n",
    "\n",
    "    def update(self, sess, s, a, y):\n",
    "        \"\"\"\n",
    "        Updates the estimator towards the given targets.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session object\n",
    "          s: State input of shape [batch_size, 4, 160, 160, 3]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          y: Targets of shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        feed_dict = { self.X_pl: s, self.y_pl: y, self.actions_pl: a }\n",
    "        summaries, global_step, _, loss = sess.run(\n",
    "            [self.summaries, tf.contrib.framework.get_global_step(), self.train_op, self.loss],\n",
    "            feed_dict)\n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summaries, global_step)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonmancuso/.local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.04209291  0.          0.01149839]\n",
      " [ 0.          0.04209291  0.          0.01149839]]\n",
      "99.465\n"
     ]
    }
   ],
   "source": [
    "# For Testing....\n",
    "\n",
    "tf.reset_default_graph()\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "e = Estimator(scope=\"test\")\n",
    "sp = StateProcessor()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Example observation batch\n",
    "    observation = env.reset()\n",
    "    \n",
    "    observation_p = sp.process(sess, observation)\n",
    "    observation = np.stack([observation_p] * 4, axis=2)\n",
    "    observations = np.array([observation] * 2)\n",
    "    \n",
    "    # Test Prediction\n",
    "    print(e.predict(sess, observations))\n",
    "\n",
    "    # Test training step\n",
    "    y = np.array([10.0, 10.0])\n",
    "    a = np.array([1, 3])\n",
    "    print(e.update(sess, observations, a, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def copy_model_parameters(sess, estimator1, estimator2):\n",
    "    \"\"\"\n",
    "    Copies the model parameters of one estimator to another.\n",
    "\n",
    "    Args:\n",
    "      sess: Tensorflow session instance\n",
    "      estimator1: Estimator to copy the paramters from\n",
    "      estimator2: Estimator to copy the parameters to\n",
    "    \"\"\"\n",
    "    e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "    e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "    e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "    e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "\n",
    "    update_ops = []\n",
    "    for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "        op = e2_v.assign(e1_v)\n",
    "        update_ops.append(op)\n",
    "\n",
    "    sess.run(update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        nA: Number of actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def deep_q_learning(sess,\n",
    "                    env,\n",
    "                    q_estimator,\n",
    "                    target_estimator,\n",
    "                    state_processor,\n",
    "                    num_episodes,\n",
    "                    experiment_dir,\n",
    "                    replay_memory_size=500000,\n",
    "                    replay_memory_init_size=50000,\n",
    "                    update_target_estimator_every=10000,\n",
    "                    discount_factor=0.99,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=500000,\n",
    "                    batch_size=32,\n",
    "                    record_video_every=1000):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm for fff-policy TD control using Function Approximation.\n",
    "    Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "\n",
    "    Args:\n",
    "        sess: Tensorflow Session object\n",
    "        env: OpenAI environment\n",
    "        q_estimator: Estimator object used for the q values\n",
    "        target_estimator: Estimator object used for the targets\n",
    "        state_processor: A StateProcessor object\n",
    "        num_episodes: Number of episodes to run for\n",
    "        experiment_dir: Directory to save Tensorflow summaries in\n",
    "        replay_memory_size: Size of the replay memory\n",
    "        replay_memory_init_size: Number of random experiences to sampel when initializing \n",
    "          the reply memory.\n",
    "        update_target_estimator_every: Copy parameters from the Q estimator to the \n",
    "          target estimator every N steps\n",
    "        discount_factor: Lambda time discount factor\n",
    "        epsilon_start: Chance to sample a random action when taking an action.\n",
    "          Epsilon is decayed over time and this is the start value\n",
    "        epsilon_end: The final minimum value of epsilon after decaying is done\n",
    "        epsilon_decay_steps: Number of steps to decay epsilon over\n",
    "        batch_size: Size of batches to sample from the replay memory\n",
    "        record_video_every: Record a video every N episodes\n",
    "\n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"successor\", \"done\"])\n",
    "\n",
    "    # The replay memory\n",
    "    replay_memory = []\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "\n",
    "    # Create directories for checkpoints and summaries\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    # Load a previous checkpoint if we find one\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "    \n",
    "    # Get the current time step\n",
    "    total_t = sess.run(tf.contrib.framework.get_global_step())\n",
    "\n",
    "    # The epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "\n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(\n",
    "        q_estimator,\n",
    "        len(VALID_ACTIONS))\n",
    "\n",
    "    # Populate the replay memory with initial experience\n",
    "    print(\"Populating replay memory...\")\n",
    "    state = env.reset()\n",
    "    state = state_processor.process(sess, state)\n",
    "    state = np.stack([state] * 4, axis=2)\n",
    "    for i in range(replay_memory_init_size):\n",
    "        action_probs = policy(sess, state, epsilons[min(total_t,epsilon_decay_steps)])\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        successor, reward, done, _ = env.step(action)\n",
    "        successor = state_processor.process(sess, successor)\n",
    "        successor = np.stack([successor] * 4, axis=2)\n",
    "        replay_memory.append(Transition(state,action,reward,successor,done))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state = state_processor.process(sess, state)\n",
    "            state = np.stack([state] * 4, axis=2)\n",
    "            break\n",
    "        else:\n",
    "            state = successor\n",
    "\n",
    "    # Record videos\n",
    "    env = Monitor(env, \n",
    "                  directory=monitor_path, \n",
    "                  video_callable=lambda count: count % record_video_every == 0, \n",
    "                  resume=True)\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        \n",
    "        # Save the current checkpoint\n",
    "        saver.save(tf.get_default_session(), checkpoint_path)\n",
    "\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        state = state_processor.process(sess, state)\n",
    "        state = np.stack([state] * 4, axis=2)\n",
    "        loss = None\n",
    "        \n",
    "        # Start timer\n",
    "        time0 = time.time()\n",
    "\n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # Epsilon for this time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "\n",
    "            # Add epsilon to Tensorboard\n",
    "            episode_summary = tf.Summary()\n",
    "            episode_summary.value.add(simple_value=epsilon, tag=\"epsilon\")\n",
    "            q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
    "\n",
    "            # Maybe update the target estimator\n",
    "            if total_t % update_target_estimator_every == 0:\n",
    "                copy_model_parameters(sess, q_estimator, target_estimator)\n",
    "\n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                    t, total_t, i_episode + 1, num_episodes, loss), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # Take a step in the environment\n",
    "            action_probs = policy(sess, state, epsilons[min(total_t,epsilon_decay_steps)])\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            successor, reward, done, _ = env.step(action)\n",
    "            successor = state_processor.process(sess, successor)\n",
    "            successor = np.stack([successor] * 4, axis=2)\n",
    "            \n",
    "            # If our replay memory is full, pop the first element\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "\n",
    "            # Save transition to replay memory\n",
    "            replay_memory.append(Transition(state,action,reward,successor,done))\n",
    "\n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "\n",
    "            # Sample a minibatch from the replay memory\n",
    "            sample = random.sample(replay_memory, batch_size)\n",
    "            state_batch, action_batch, reward_batch, successor_batch, done_batch = map(np.array,zip(*sample))\n",
    "            \n",
    "            # Calculate q values and targets\n",
    "            next_q_values = q_estimator.predict(sess, state_batch)\n",
    "            best_actions = np.argmax(next_q_values, axis=1)\n",
    "            next_q_values_target = target_estimator.predict(sess, state_batch)\n",
    "            target_batch = reward_batch+np.invert(done_batch).astype(np.float32)*discount_factor*np.amax(next_q_values_target[:,best_actions], axis=1)\n",
    "            \n",
    "            # Perform gradient descent update\n",
    "            states_batch = np.array(state_batch)\n",
    "            loss = q_estimator.update(sess, state_batch, action_batch, target_batch)\n",
    "\n",
    "            if done:\n",
    "                print(\"\\nEpisode time: \",time.time()-time0)\n",
    "                break\n",
    "\n",
    "            state = successor\n",
    "            total_t += 1\n",
    "\n",
    "        # Add summaries to tensorboard\n",
    "        episode_summary = tf.Summary()\n",
    "        episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], \n",
    "                                  node_name=\"episode_reward\", tag=\"episode_reward\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], \n",
    "                                  node_name=\"episode_length\", tag=\"episode_length\")\n",
    "        q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
    "        q_estimator.summary_writer.flush()\n",
    "\n",
    "        yield total_t, plotting.EpisodeStats(\n",
    "            episode_lengths=stats.episode_lengths[:i_episode+1],\n",
    "            episode_rewards=stats.episode_rewards[:i_episode+1])\n",
    "\n",
    "    env.monitor.close()\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonmancuso/.local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "[2017-05-17 18:34:16,756] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/jasonmancuso/reinforcement-learning/DQN/experiments/Breakout-v0/monitor')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model checkpoint /Users/jasonmancuso/reinforcement-learning/DQN/experiments/Breakout-v0/checkpoints/model...\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from /Users/jasonmancuso/reinforcement-learning/DQN/experiments/Breakout-v0/checkpoints/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-17 18:34:17,490] Restoring parameters from /Users/jasonmancuso/reinforcement-learning/DQN/experiments/Breakout-v0/checkpoints/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating replay memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-17 18:34:19,929] Starting new video recorder writing to /Users/jasonmancuso/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.8.31289.video000000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 198 (420290) @ Episode 1/10000, loss: 0.01902422681450843805\n",
      "Episode time:  21.82379698753357\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 372 (420662) @ Episode 2/10000, loss: 0.00024102447787299752\n",
      "Episode time:  42.97931218147278\n",
      "\n",
      "Episode Reward: 3.0\n",
      "Step 403 (421065) @ Episode 3/10000, loss: 0.00022045813966542482\n",
      "Episode time:  47.20883774757385\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 305 (421370) @ Episode 4/10000, loss: 0.00019242048438172787\n",
      "Episode time:  39.05564212799072\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 187 (421557) @ Episode 5/10000, loss: 0.00011886189895449206\n",
      "Episode time:  21.848612308502197\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 316 (421873) @ Episode 6/10000, loss: 0.0798746719956398-055\n",
      "Episode time:  37.54846715927124\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 183 (422056) @ Episode 7/10000, loss: 0.00011401355732232332\n",
      "Episode time:  22.75755786895752\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 164 (422220) @ Episode 8/10000, loss: 0.00359612936154007975\n",
      "Episode time:  19.660347938537598\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 318 (422538) @ Episode 9/10000, loss: 4.3569445551838726e-05\n",
      "Episode time:  38.32252287864685\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 314 (422852) @ Episode 10/10000, loss: 5.182044333196245e-053\n",
      "Episode time:  36.26746892929077\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 187 (423039) @ Episode 11/10000, loss: 2.599863728391938e-055\n",
      "Episode time:  21.516804218292236\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 244 (423283) @ Episode 12/10000, loss: 7.045127858873457e-058\n",
      "Episode time:  30.406351804733276\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 299 (423582) @ Episode 13/10000, loss: 0.00014481302059721202\n",
      "Episode time:  34.48248529434204\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 246 (423828) @ Episode 14/10000, loss: 0.00056691828649491077\n",
      "Episode time:  28.482553005218506\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 299 (424127) @ Episode 15/10000, loss: 0.01240488421171903652\n",
      "Episode time:  34.82565212249756\n",
      "\n",
      "Episode Reward: 3.0\n",
      "Step 257 (424384) @ Episode 16/10000, loss: 0.00220826314762234754\n",
      "Episode time:  29.752912044525146\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 268 (424652) @ Episode 17/10000, loss: 0.00015712148160673678\n",
      "Episode time:  31.007557153701782\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 234 (424886) @ Episode 18/10000, loss: 0.00051202590111643083\n",
      "Episode time:  27.048504114151\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 159 (425045) @ Episode 19/10000, loss: 0.00524550769478082746\n",
      "Episode time:  18.628580808639526\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 231 (425276) @ Episode 20/10000, loss: 0.00012621772475540638\n",
      "Episode time:  26.905858039855957\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 299 (425575) @ Episode 21/10000, loss: 0.00038728694198653134\n",
      "Episode time:  34.8819739818573\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 225 (425800) @ Episode 22/10000, loss: 0.00011280018225079402\n",
      "Episode time:  26.183730363845825\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 203 (426003) @ Episode 23/10000, loss: 0.00030747262644581497\n",
      "Episode time:  23.69789695739746\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 288 (426291) @ Episode 24/10000, loss: 0.00098730705212801721\n",
      "Episode time:  36.329797983169556\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 170 (426461) @ Episode 25/10000, loss: 0.00080338231055065994\n",
      "Episode time:  22.10928988456726\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 305 (426766) @ Episode 26/10000, loss: 0.00030530616641044617\n",
      "Episode time:  41.90124797821045\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 541 (427307) @ Episode 27/10000, loss: 0.00011074858048232272\n",
      "Episode time:  72.01964282989502\n",
      "\n",
      "Episode Reward: 6.0\n",
      "Step 172 (427479) @ Episode 28/10000, loss: 8.705584332346916e-057\n",
      "Episode time:  23.319710969924927\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 232 (427711) @ Episode 29/10000, loss: 7.116844790289178e-054\n",
      "Episode time:  30.148438930511475\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 241 (427952) @ Episode 30/10000, loss: 0.00025910040130838753\n",
      "Episode time:  30.24645209312439\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 420 (428372) @ Episode 31/10000, loss: 0.04156360775232315465\n",
      "Episode time:  50.74755525588989\n",
      "\n",
      "Episode Reward: 4.0\n",
      "Step 304 (428676) @ Episode 32/10000, loss: 0.00458778766915202198\n",
      "Episode time:  36.67883014678955\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 227 (428903) @ Episode 33/10000, loss: 0.05548625066876411486\n",
      "Episode time:  28.077541828155518\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 304 (429207) @ Episode 34/10000, loss: 0.00022154541511554275\n",
      "Episode time:  37.212080240249634\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 261 (429468) @ Episode 35/10000, loss: 0.00141199491918087484\n",
      "Episode time:  31.007532119750977\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 250 (429718) @ Episode 36/10000, loss: 7.544846448581666e-055\n",
      "Episode time:  29.999136924743652\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 236 (429954) @ Episode 37/10000, loss: 0.00201663654297590267\n",
      "Episode time:  27.90643310546875\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 270 (430224) @ Episode 38/10000, loss: 0.09664127230644226198\n",
      "Episode time:  31.804553031921387\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 207 (430431) @ Episode 39/10000, loss: 0.00022048270329833034\n",
      "Episode time:  24.169975757598877\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 190 (430621) @ Episode 40/10000, loss: 0.00012421022984199226\n",
      "Episode time:  22.407344818115234\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 285 (430906) @ Episode 41/10000, loss: 4.9358492105966434e-05\n",
      "Episode time:  33.47296214103699\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 186 (431092) @ Episode 42/10000, loss: 0.00012604237417690456\n",
      "Episode time:  22.02156090736389\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 166 (431258) @ Episode 43/10000, loss: 0.00102194352075457579\n",
      "Episode time:  19.5693039894104\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 286 (431544) @ Episode 44/10000, loss: 0.00114853738341480587\n",
      "Episode time:  33.68996596336365\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 197 (431741) @ Episode 45/10000, loss: 0.00090874184388667354\n",
      "Episode time:  23.25594687461853\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 427 (432168) @ Episode 46/10000, loss: 0.00591587647795677276\n",
      "Episode time:  50.45229125022888\n",
      "\n",
      "Episode Reward: 5.0\n",
      "Step 223 (432391) @ Episode 47/10000, loss: 0.00031302406569011518\n",
      "Episode time:  25.528100728988647\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 298 (432689) @ Episode 48/10000, loss: 0.00020546656742226332\n",
      "Episode time:  33.65264391899109\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 256 (432945) @ Episode 49/10000, loss: 0.00014198693679645658\n",
      "Episode time:  28.79043698310852\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 310 (433255) @ Episode 50/10000, loss: 0.00330017809756100185\n",
      "Episode time:  34.865723848342896\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 257 (433512) @ Episode 51/10000, loss: 0.01041945721954107347\n",
      "Episode time:  30.154693126678467\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 194 (433706) @ Episode 52/10000, loss: 0.00085561198648065332\n",
      "Episode time:  23.226502180099487\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 170 (433876) @ Episode 53/10000, loss: 0.00017599972488824278\n",
      "Episode time:  20.755468130111694\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 351 (434227) @ Episode 54/10000, loss: 0.00340762245468795326\n",
      "Episode time:  42.03791284561157\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 201 (434428) @ Episode 55/10000, loss: 0.00467162253335118349\n",
      "Episode time:  23.96926212310791\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 298 (434726) @ Episode 56/10000, loss: 0.00553332595154643155\n",
      "Episode time:  35.29354786872864\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 290 (435016) @ Episode 57/10000, loss: 0.05446236953139305828\n",
      "Episode time:  34.10585880279541\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 232 (435248) @ Episode 58/10000, loss: 0.00014616297266911715\n",
      "Episode time:  27.35331916809082\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 181 (435429) @ Episode 59/10000, loss: 0.00011226352944504472\n",
      "Episode time:  21.353396892547607\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 349 (435778) @ Episode 60/10000, loss: 0.00031356545514427125\n",
      "Episode time:  41.429967164993286\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 293 (436071) @ Episode 61/10000, loss: 0.00021585017384495586\n",
      "Episode time:  35.128397703170776\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 300 (436371) @ Episode 62/10000, loss: 0.00010423734784126282\n",
      "Episode time:  36.084218978881836\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 194 (436565) @ Episode 63/10000, loss: 0.00023748747480567545\n",
      "Episode time:  22.45845675468445\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 239 (436804) @ Episode 64/10000, loss: 0.00011829018330899999\n",
      "Episode time:  27.032124996185303\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 286 (437090) @ Episode 65/10000, loss: 0.00041910231811925778\n",
      "Episode time:  33.501850843429565\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 350 (437440) @ Episode 66/10000, loss: 0.00036801697569899264\n",
      "Episode time:  43.55696892738342\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 179 (437619) @ Episode 67/10000, loss: 0.00036017593811266124\n",
      "Episode time:  22.649733066558838\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 171 (437790) @ Episode 68/10000, loss: 0.00017748971004039057\n",
      "Episode time:  21.672291040420532\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 683 (438473) @ Episode 69/10000, loss: 0.00021838056272827095\n",
      "Episode time:  86.59383893013\n",
      "\n",
      "Episode Reward: 9.0\n",
      "Step 251 (438724) @ Episode 70/10000, loss: 0.00031875344575382773\n",
      "Episode time:  31.24532985687256\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 266 (438990) @ Episode 71/10000, loss: 0.02977405674755573376\n",
      "Episode time:  32.74749231338501\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 356 (439346) @ Episode 72/10000, loss: 0.00021494692191481595\n",
      "Episode time:  44.079447746276855\n",
      "\n",
      "Episode Reward: 3.0\n",
      "Step 362 (439708) @ Episode 73/10000, loss: 0.00023072486510500312\n",
      "Episode time:  43.658961057662964\n",
      "\n",
      "Episode Reward: 3.0\n",
      "Step 507 (440215) @ Episode 74/10000, loss: 0.00049631990259513267\n",
      "Episode time:  60.97068428993225\n",
      "\n",
      "Episode Reward: 4.0\n",
      "Step 224 (440439) @ Episode 75/10000, loss: 0.00015414718654938042\n",
      "Episode time:  26.75787925720215\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 231 (440670) @ Episode 76/10000, loss: 0.00017857673810794954\n",
      "Episode time:  27.75966501235962\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 509 (441179) @ Episode 77/10000, loss: 0.00329418783076107575\n",
      "Episode time:  60.78466296195984\n",
      "\n",
      "Episode Reward: 4.0\n",
      "Step 191 (441370) @ Episode 78/10000, loss: 0.00012120949395466596\n",
      "Episode time:  22.985963106155396\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 346 (441716) @ Episode 79/10000, loss: 0.01365664787590503728\n",
      "Episode time:  41.229241132736206\n",
      "\n",
      "Episode Reward: 3.0\n",
      "Step 218 (441934) @ Episode 80/10000, loss: 0.00022027801605872815\n",
      "Episode time:  24.96737504005432\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 220 (442154) @ Episode 81/10000, loss: 0.00015185945085249845\n",
      "Episode time:  24.81061816215515\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 315 (442469) @ Episode 82/10000, loss: 0.00029170012567192316\n",
      "Episode time:  35.46835684776306\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 289 (442758) @ Episode 83/10000, loss: 9.203972149407491e-059\n",
      "Episode time:  32.44052004814148\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 268 (443026) @ Episode 84/10000, loss: 0.00442916387692093854\n",
      "Episode time:  30.232646226882935\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 299 (443325) @ Episode 85/10000, loss: 0.00014205880870576948\n",
      "Episode time:  33.630309104919434\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 429 (443754) @ Episode 86/10000, loss: 0.00101210980210453275\n",
      "Episode time:  48.12518763542175\n",
      "\n",
      "Episode Reward: 4.0\n",
      "Step 170 (443924) @ Episode 87/10000, loss: 0.00018683377129491419\n",
      "Episode time:  19.099414110183716\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 260 (444184) @ Episode 88/10000, loss: 0.00041647808393463497\n",
      "Episode time:  29.28731393814087\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 219 (444403) @ Episode 89/10000, loss: 0.00014758406905457377\n",
      "Episode time:  24.5524799823761\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 220 (444623) @ Episode 90/10000, loss: 0.00043532028212212026\n",
      "Episode time:  24.68884015083313\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 491 (445114) @ Episode 91/10000, loss: 0.00062451028497889646\n",
      "Episode time:  55.13599419593811\n",
      "\n",
      "Episode Reward: 5.0\n",
      "Step 230 (445344) @ Episode 92/10000, loss: 0.01038078032433986798\n",
      "Episode time:  25.877570152282715\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 248 (445592) @ Episode 93/10000, loss: 0.00044037654879502958\n",
      "Episode time:  27.857375144958496\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 373 (445965) @ Episode 94/10000, loss: 0.00923705473542213457\n",
      "Episode time:  41.81352400779724\n",
      "\n",
      "Episode Reward: 3.0\n",
      "Step 249 (446214) @ Episode 95/10000, loss: 0.02089916169643402722\n",
      "Episode time:  27.90659499168396\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 175 (446389) @ Episode 96/10000, loss: 0.00037114275619387627\n",
      "Episode time:  19.646536827087402\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 346 (446735) @ Episode 97/10000, loss: 0.05447805300354957623\n",
      "Episode time:  38.81315493583679\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 229 (446964) @ Episode 98/10000, loss: 0.00046564757940359414\n",
      "Episode time:  25.766067028045654\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 248 (447212) @ Episode 99/10000, loss: 0.00022021416225470603\n",
      "Episode time:  27.9435818195343\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 177 (447389) @ Episode 100/10000, loss: 0.00023352517746388912\n",
      "Episode time:  19.836987018585205\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 618 (448007) @ Episode 101/10000, loss: 0.00020041098468936983\n",
      "Episode time:  69.33785605430603\n",
      "\n",
      "Episode Reward: 7.0\n",
      "Step 277 (448284) @ Episode 102/10000, loss: 0.00077900569885969168\n",
      "Episode time:  31.293598890304565\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 235 (448519) @ Episode 103/10000, loss: 7.966339035192505e-057\n",
      "Episode time:  26.423950910568237\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 252 (448771) @ Episode 104/10000, loss: 0.00063441577367484577\n",
      "Episode time:  28.346533060073853\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 192 (448963) @ Episode 105/10000, loss: 9.78530733846128e-0553\n",
      "Episode time:  21.52537703514099\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 168 (449131) @ Episode 106/10000, loss: 0.00268833246082067505\n",
      "Episode time:  18.892213821411133\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 261 (449392) @ Episode 107/10000, loss: 0.00011007521243300289\n",
      "Episode time:  29.278245210647583\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 253 (449645) @ Episode 108/10000, loss: 0.00011014466872438788\n",
      "Episode time:  28.371639013290405\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 254 (449899) @ Episode 109/10000, loss: 3.92215843021404e-0537\n",
      "Episode time:  28.507917165756226\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 185 (450084) @ Episode 110/10000, loss: 0.03682286292314529487\n",
      "Episode time:  20.815490007400513\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 217 (450301) @ Episode 111/10000, loss: 0.03488796204328537467\n",
      "Episode time:  24.38322615623474\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 211 (450512) @ Episode 112/10000, loss: 0.00019748962949961424\n",
      "Episode time:  23.642077922821045\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 273 (450785) @ Episode 113/10000, loss: 0.00518479896709322972\n",
      "Episode time:  30.702165842056274\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 177 (450962) @ Episode 114/10000, loss: 0.00015463652380276471\n",
      "Episode time:  19.8981990814209\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 224 (451186) @ Episode 115/10000, loss: 0.00719041842967271854\n",
      "Episode time:  25.223267078399658\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 211 (451397) @ Episode 116/10000, loss: 0.00011016467760782689\n",
      "Episode time:  24.18716812133789\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 206 (451603) @ Episode 117/10000, loss: 7.732711674179882e-051\n",
      "Episode time:  23.555158138275146\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 260 (451863) @ Episode 118/10000, loss: 0.00015530963719356805\n",
      "Episode time:  29.332629680633545\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 251 (452114) @ Episode 119/10000, loss: 0.00449686544016003696\n",
      "Episode time:  28.19120502471924\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 265 (452379) @ Episode 120/10000, loss: 9.614742884878069e-057\n",
      "Episode time:  29.89206576347351\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 250 (452629) @ Episode 121/10000, loss: 0.00018381065456196666\n",
      "Episode time:  28.062479257583618\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 244 (452873) @ Episode 122/10000, loss: 0.00043773849029093984\n",
      "Episode time:  27.42518711090088\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 164 (453037) @ Episode 123/10000, loss: 0.00015754325431771576\n",
      "Episode time:  18.46115779876709\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 392 (453429) @ Episode 124/10000, loss: 0.01685600727796554654\n",
      "Episode time:  44.088884115219116\n",
      "\n",
      "Episode Reward: 3.0\n",
      "Step 404 (453833) @ Episode 125/10000, loss: 0.00036930962232872844\n",
      "Episode time:  45.42770004272461\n",
      "\n",
      "Episode Reward: 3.0\n",
      "Step 273 (454106) @ Episode 126/10000, loss: 0.00010031866258941591\n",
      "Episode time:  30.691030025482178\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 352 (454458) @ Episode 127/10000, loss: 0.00729795545339584352\n",
      "Episode time:  39.60267090797424\n",
      "\n",
      "Episode Reward: 3.0\n",
      "Step 218 (454676) @ Episode 128/10000, loss: 0.00072479981463402519\n",
      "Episode time:  24.52705192565918\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 194 (454870) @ Episode 129/10000, loss: 0.00157585204578936152\n",
      "Episode time:  21.848968982696533\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 183 (455053) @ Episode 130/10000, loss: 0.00011028436711058023\n",
      "Episode time:  20.661896228790283\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 179 (455232) @ Episode 131/10000, loss: 0.00201919861137866975\n",
      "Episode time:  20.21303701400757\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 307 (455539) @ Episode 132/10000, loss: 0.00031701018451713026\n",
      "Episode time:  34.628366231918335\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 182 (455721) @ Episode 133/10000, loss: 0.00023596511164214462\n",
      "Episode time:  20.507074117660522\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 178 (455899) @ Episode 134/10000, loss: 0.05566892772912979944\n",
      "Episode time:  20.08326506614685\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 242 (456141) @ Episode 135/10000, loss: 0.00511331483721733103\n",
      "Episode time:  27.30963683128357\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 182 (456323) @ Episode 136/10000, loss: 0.00038915383629500866\n",
      "Episode time:  20.461094856262207\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 365 (456688) @ Episode 137/10000, loss: 0.05636983364820489915\n",
      "Episode time:  41.13611078262329\n",
      "\n",
      "Episode Reward: 3.0\n",
      "Step 256 (456944) @ Episode 138/10000, loss: 0.24061936140060425573\n",
      "Episode time:  28.76003384590149\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 384 (457328) @ Episode 139/10000, loss: 0.00030392321059480315\n",
      "Episode time:  43.47134971618652\n",
      "\n",
      "Episode Reward: 3.0\n",
      "Step 217 (457545) @ Episode 140/10000, loss: 0.00532227382063865773\n",
      "Episode time:  24.51477599143982\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 462 (458007) @ Episode 141/10000, loss: 0.00040597669431008497\n",
      "Episode time:  52.046923875808716\n",
      "\n",
      "Episode Reward: 5.0\n",
      "Step 274 (458281) @ Episode 142/10000, loss: 0.00032345080398954454\n",
      "Episode time:  30.907905101776123\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 263 (458544) @ Episode 143/10000, loss: 0.00017484827549196787\n",
      "Episode time:  29.662894248962402\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 218 (458762) @ Episode 144/10000, loss: 0.08731438964605331322\n",
      "Episode time:  24.58596897125244\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 156 (458918) @ Episode 145/10000, loss: 0.00022730117780156434\n",
      "Episode time:  17.755024194717407\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 248 (459166) @ Episode 146/10000, loss: 0.00425032200291752873\n",
      "Episode time:  27.990759134292603\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 233 (459399) @ Episode 147/10000, loss: 0.00087070651352405556\n",
      "Episode time:  26.42329692840576\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 190 (459589) @ Episode 148/10000, loss: 0.00035094568738713863\n",
      "Episode time:  21.41470193862915\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 466 (460055) @ Episode 149/10000, loss: 0.00042362016392871747\n",
      "Episode time:  51.9419047832489\n",
      "\n",
      "Episode Reward: 5.0\n",
      "Step 254 (460309) @ Episode 150/10000, loss: 0.00029356472077779475\n",
      "Episode time:  28.421128034591675\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 256 (460565) @ Episode 151/10000, loss: 0.00031055838917382064\n",
      "Episode time:  28.836770057678223\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 374 (460939) @ Episode 152/10000, loss: 0.00150507327634841256\n",
      "Episode time:  47.91973686218262\n",
      "\n",
      "Episode Reward: 4.0\n",
      "Step 280 (461219) @ Episode 153/10000, loss: 0.00057743408251553774\n",
      "Episode time:  35.77774906158447\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 291 (461510) @ Episode 154/10000, loss: 0.00017893659241963178\n",
      "Episode time:  41.60464096069336\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 258 (461768) @ Episode 155/10000, loss: 0.00100827077403664595\n",
      "Episode time:  34.09697699546814\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 182 (461950) @ Episode 156/10000, loss: 0.00121164543088525537\n",
      "Episode time:  23.869714975357056\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 326 (462276) @ Episode 157/10000, loss: 0.00015908300701994456\n",
      "Episode time:  42.323649168014526\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 187 (462463) @ Episode 158/10000, loss: 0.00184762850403785765\n",
      "Episode time:  24.061066150665283\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 306 (462769) @ Episode 159/10000, loss: 0.00010780027514556423\n",
      "Episode time:  39.23670029640198\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 245 (463014) @ Episode 160/10000, loss: 0.00015691680891904988\n",
      "Episode time:  31.171745777130127\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 213 (463227) @ Episode 161/10000, loss: 0.00052316475193947553\n",
      "Episode time:  27.300665140151978\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 309 (463536) @ Episode 162/10000, loss: 5.2302435506135225e-05\n",
      "Episode time:  39.6757071018219\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 272 (463808) @ Episode 163/10000, loss: 0.00021248782286420465\n",
      "Episode time:  36.095211029052734\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 193 (464001) @ Episode 164/10000, loss: 0.03427625447511673264\n",
      "Episode time:  25.54276990890503\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 307 (464308) @ Episode 165/10000, loss: 0.00057980907149612965\n",
      "Episode time:  40.64825892448425\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 404 (464712) @ Episode 166/10000, loss: 0.01807303167879581537\n",
      "Episode time:  53.95004487037659\n",
      "\n",
      "Episode Reward: 3.0\n",
      "Step 242 (464954) @ Episode 167/10000, loss: 0.0644875317811966-057\n",
      "Episode time:  32.81486105918884\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 283 (465237) @ Episode 168/10000, loss: 0.00019709554908331484\n",
      "Episode time:  38.49136829376221\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 246 (465483) @ Episode 169/10000, loss: 0.00043427734635770323\n",
      "Episode time:  33.78352475166321\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 263 (465746) @ Episode 170/10000, loss: 0.2105873078107834e-05\n",
      "Episode time:  35.8312509059906\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 251 (465997) @ Episode 171/10000, loss: 0.00020708788360934705\n",
      "Episode time:  34.32795786857605\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 247 (466244) @ Episode 172/10000, loss: 0.00429660733789205555\n",
      "Episode time:  34.06807780265808\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 187 (466431) @ Episode 173/10000, loss: 0.24324414134025574404\n",
      "Episode time:  25.760640144348145\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 509 (466940) @ Episode 174/10000, loss: 0.08721546083688736285\n",
      "Episode time:  70.22206282615662\n",
      "\n",
      "Episode Reward: 5.0\n",
      "Step 292 (467232) @ Episode 175/10000, loss: 0.00039839895907789475\n",
      "Episode time:  40.17013716697693\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 304 (467536) @ Episode 176/10000, loss: 0.00017554959049448375\n",
      "Episode time:  42.11390805244446\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 356 (467892) @ Episode 177/10000, loss: 0.00017453684995416552\n",
      "Episode time:  49.62414813041687\n",
      "\n",
      "Episode Reward: 3.0\n",
      "Step 224 (468116) @ Episode 178/10000, loss: 0.00024053160450421274\n",
      "Episode time:  31.122840881347656\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 266 (468382) @ Episode 179/10000, loss: 0.08629558235406876834\n",
      "Episode time:  37.37730407714844\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 375 (468757) @ Episode 180/10000, loss: 0.00013695625239051878\n",
      "Episode time:  51.98043632507324\n",
      "\n",
      "Episode Reward: 4.0\n",
      "Step 232 (468989) @ Episode 181/10000, loss: 0.00035340117756277323\n",
      "Episode time:  32.29082489013672\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 242 (469231) @ Episode 182/10000, loss: 0.00054574979003518829\n",
      "Episode time:  33.78483510017395\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 251 (469482) @ Episode 183/10000, loss: 0.00037584200617857282\n",
      "Episode time:  35.160706996917725\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 246 (469728) @ Episode 184/10000, loss: 0.01067145075649023946\n",
      "Episode time:  34.67405319213867\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 289 (470017) @ Episode 185/10000, loss: 0.00037717109080404043\n",
      "Episode time:  40.98045778274536\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 231 (470248) @ Episode 186/10000, loss: 0.00017162247968371958\n",
      "Episode time:  32.650943994522095\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 416 (470664) @ Episode 187/10000, loss: 0.00034115632297471166\n",
      "Episode time:  58.81484007835388\n",
      "\n",
      "Episode Reward: 4.0\n",
      "Step 349 (471013) @ Episode 188/10000, loss: 0.00510667776688933455\n",
      "Episode time:  49.11716389656067\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 256 (471269) @ Episode 189/10000, loss: 0.00215180963277816773\n",
      "Episode time:  36.004960775375366\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 226 (471495) @ Episode 190/10000, loss: 0.00021492880478035659\n",
      "Episode time:  31.877856016159058\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 262 (471757) @ Episode 191/10000, loss: 0.00050651677884161473\n",
      "Episode time:  37.179091930389404\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 189 (471946) @ Episode 192/10000, loss: 0.00036664481740444926\n",
      "Episode time:  26.75345206260681\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 203 (472149) @ Episode 193/10000, loss: 0.00934158358722925223\n",
      "Episode time:  29.630565881729126\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 220 (472369) @ Episode 194/10000, loss: 0.00011997138790320605\n",
      "Episode time:  30.282225131988525\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 292 (472661) @ Episode 195/10000, loss: 0.00019838471780531108\n",
      "Episode time:  40.41311597824097\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 358 (473019) @ Episode 196/10000, loss: 0.00016538769705221057\n",
      "Episode time:  49.44319701194763\n",
      "\n",
      "Episode Reward: 3.0\n",
      "Step 376 (473395) @ Episode 197/10000, loss: 0.00018856796668842435\n",
      "Episode time:  52.18102788925171\n",
      "\n",
      "Episode Reward: 3.0\n",
      "Step 329 (473724) @ Episode 198/10000, loss: 0.00459594046697020566\n",
      "Episode time:  45.546168088912964\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 172 (473896) @ Episode 199/10000, loss: 0.00042352970922365785\n",
      "Episode time:  24.554045915603638\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 189 (474085) @ Episode 200/10000, loss: 0.00022571066801901907\n",
      "Episode time:  27.54305386543274\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 258 (474343) @ Episode 201/10000, loss: 0.00069069460732862357\n",
      "Episode time:  35.87938404083252\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 295 (474638) @ Episode 202/10000, loss: 0.00128926523029804237\n",
      "Episode time:  41.1231689453125\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 245 (474883) @ Episode 203/10000, loss: 0.00029909174190834165\n",
      "Episode time:  33.85925793647766\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 247 (475130) @ Episode 204/10000, loss: 0.00095420586876571186\n",
      "Episode time:  33.8545880317688\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 282 (475412) @ Episode 205/10000, loss: 0.00024251555441878736\n",
      "Episode time:  38.93949103355408\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 333 (475745) @ Episode 206/10000, loss: 0.00029561002156697214\n",
      "Episode time:  45.486879110336304\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 483 (476228) @ Episode 207/10000, loss: 0.00034615976619534194\n",
      "Episode time:  67.12572312355042\n",
      "\n",
      "Episode Reward: 6.0\n",
      "Step 351 (476579) @ Episode 208/10000, loss: 0.00381504977121949252\n",
      "Episode time:  48.96211004257202\n",
      "\n",
      "Episode Reward: 3.0\n",
      "Step 378 (476957) @ Episode 209/10000, loss: 0.03857984021306038414\n",
      "Episode time:  54.74558186531067\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 257 (477214) @ Episode 210/10000, loss: 0.00026773859281092884\n",
      "Episode time:  37.26602077484131\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 382 (477596) @ Episode 211/10000, loss: 0.00368276913650333984\n",
      "Episode time:  55.57798504829407\n",
      "\n",
      "Episode Reward: 3.0\n",
      "Step 389 (477985) @ Episode 212/10000, loss: 0.00076272396836429839\n",
      "Episode time:  55.824005126953125\n",
      "\n",
      "Episode Reward: 4.0\n",
      "Step 177 (478162) @ Episode 213/10000, loss: 0.00023174825764726847\n",
      "Episode time:  25.54960012435913\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 271 (478433) @ Episode 214/10000, loss: 0.00027222893550060694\n",
      "Episode time:  39.52677321434021\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 266 (478699) @ Episode 215/10000, loss: 0.00090570794418454172\n",
      "Episode time:  39.03314423561096\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 260 (478959) @ Episode 216/10000, loss: 0.00022888922831043627\n",
      "Episode time:  38.163017988204956\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 183 (479142) @ Episode 217/10000, loss: 0.00074309622868895535\n",
      "Episode time:  26.48292589187622\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 284 (479426) @ Episode 218/10000, loss: 0.00013797523570246994\n",
      "Episode time:  41.351433992385864\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 326 (479752) @ Episode 219/10000, loss: 0.00074428226798772815\n",
      "Episode time:  47.473777294158936\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 393 (480145) @ Episode 220/10000, loss: 9.195868915412575e-053\n",
      "Episode time:  57.77517008781433\n",
      "\n",
      "Episode Reward: 3.0\n",
      "Step 298 (480443) @ Episode 221/10000, loss: 0.00042949791532009843\n",
      "Episode time:  43.25638675689697\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 228 (480671) @ Episode 222/10000, loss: 0.00594513351097703495\n",
      "Episode time:  32.95665407180786\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 175 (480846) @ Episode 223/10000, loss: 2.149491774616763e-053\n",
      "Episode time:  25.993396997451782\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 409 (481255) @ Episode 224/10000, loss: 0.00022195003111846745\n",
      "Episode time:  60.07450294494629\n",
      "\n",
      "Episode Reward: 4.0\n",
      "Step 321 (481576) @ Episode 225/10000, loss: 0.00043690338497981438\n",
      "Episode time:  47.27296209335327\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 433 (482009) @ Episode 226/10000, loss: 0.00048640638124197722\n",
      "Episode time:  63.85338807106018\n",
      "\n",
      "Episode Reward: 5.0\n",
      "Step 272 (482281) @ Episode 227/10000, loss: 0.00150073785334825528\n",
      "Episode time:  39.74164414405823\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 294 (482575) @ Episode 228/10000, loss: 0.00220053037628531464\n",
      "Episode time:  43.25302195549011\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 175 (482750) @ Episode 229/10000, loss: 0.00752712227404117695\n",
      "Episode time:  25.419261932373047\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 233 (482983) @ Episode 230/10000, loss: 0.16775713860988617485\n",
      "Episode time:  34.12815308570862\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 434 (483417) @ Episode 231/10000, loss: 0.00020283422782085836\n",
      "Episode time:  63.226608991622925\n",
      "\n",
      "Episode Reward: 3.0\n",
      "Step 312 (483729) @ Episode 232/10000, loss: 0.00681737484410405244\n",
      "Episode time:  45.483978033065796\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 310 (484039) @ Episode 233/10000, loss: 0.00132460484746843587\n",
      "Episode time:  46.13111114501953\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 305 (484344) @ Episode 234/10000, loss: 0.00024644145742058754\n",
      "Episode time:  45.20003700256348\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 247 (484591) @ Episode 235/10000, loss: 9.087941725738347e-054\n",
      "Episode time:  36.521742820739746\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 241 (484832) @ Episode 236/10000, loss: 0.00437346287071704991\n",
      "Episode time:  36.079081296920776\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 358 (485190) @ Episode 237/10000, loss: 0.00021631483105011284\n",
      "Episode time:  52.81951284408569\n",
      "\n",
      "Episode Reward: 3.0\n",
      "Step 333 (485523) @ Episode 238/10000, loss: 0.00048095808597281575\n",
      "Episode time:  49.197051763534546\n",
      "\n",
      "Episode Reward: 3.0\n",
      "Step 410 (485933) @ Episode 239/10000, loss: 0.00016341752780135723\n",
      "Episode time:  60.56668305397034\n",
      "\n",
      "Episode Reward: 3.0\n",
      "Step 397 (486330) @ Episode 240/10000, loss: 0.01207241602241993297\n",
      "Episode time:  58.64385771751404\n",
      "\n",
      "Episode Reward: 4.0\n",
      "Step 292 (486622) @ Episode 241/10000, loss: 0.00920964777469635966\n",
      "Episode time:  43.61508798599243\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 300 (486922) @ Episode 242/10000, loss: 0.00067320908419787886\n",
      "Episode time:  44.39830708503723\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 335 (487257) @ Episode 243/10000, loss: 0.00371955102309584683\n",
      "Episode time:  49.98383116722107\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 237 (487494) @ Episode 244/10000, loss: 0.00045129057252779603\n",
      "Episode time:  35.178136110305786\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 224 (487718) @ Episode 245/10000, loss: 0.00326573709025979046\n",
      "Episode time:  33.32850217819214\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 277 (487995) @ Episode 246/10000, loss: 0.00049423321615904575\n",
      "Episode time:  40.13342308998108\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 588 (488583) @ Episode 247/10000, loss: 0.03276013210415841715\n",
      "Episode time:  86.7720787525177\n",
      "\n",
      "Episode Reward: 6.0\n",
      "Step 285 (488868) @ Episode 248/10000, loss: 0.00044617557432502518\n",
      "Episode time:  42.04284381866455\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 284 (489152) @ Episode 249/10000, loss: 0.00148927734699100267\n",
      "Episode time:  41.981451988220215\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 228 (489380) @ Episode 250/10000, loss: 0.00152965565212070945\n",
      "Episode time:  33.70183801651001\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 185 (489565) @ Episode 251/10000, loss: 0.00030159170273691416\n",
      "Episode time:  27.183938026428223\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 349 (489914) @ Episode 252/10000, loss: 0.00022507268295157795\n",
      "Episode time:  52.115516901016235\n",
      "\n",
      "Episode Reward: 3.0\n",
      "Step 208 (490122) @ Episode 253/10000, loss: 0.00535320397466421146\n",
      "Episode time:  31.382399797439575\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 204 (490326) @ Episode 254/10000, loss: 0.00027966947527602315\n",
      "Episode time:  30.47170615196228\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 264 (490590) @ Episode 255/10000, loss: 0.00042134016985073686\n",
      "Episode time:  39.74941897392273\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 360 (490950) @ Episode 256/10000, loss: 0.12635390460491184595\n",
      "Episode time:  54.26527285575867\n",
      "\n",
      "Episode Reward: 3.0\n",
      "Step 307 (491257) @ Episode 257/10000, loss: 0.02446412295103073056\n",
      "Episode time:  46.60104703903198\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 229 (491486) @ Episode 258/10000, loss: 0.00037421809975057848\n",
      "Episode time:  34.735259771347046\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 288 (491774) @ Episode 259/10000, loss: 0.00068393023684620866\n",
      "Episode time:  43.56195521354675\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 234 (492008) @ Episode 260/10000, loss: 0.00181873666588217025\n",
      "Episode time:  35.22574305534363\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 632 (492640) @ Episode 261/10000, loss: 0.00576260220259428456\n",
      "Episode time:  94.80125403404236\n",
      "\n",
      "Episode Reward: 9.0\n",
      "Step 185 (492825) @ Episode 262/10000, loss: 0.00322658033110201366\n",
      "Episode time:  28.109620094299316\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 424 (493249) @ Episode 263/10000, loss: 0.00228364043869078163\n",
      "Episode time:  63.55035996437073\n",
      "\n",
      "Episode Reward: 4.0\n",
      "Step 271 (493520) @ Episode 264/10000, loss: 7.499922503484413e-055\n",
      "Episode time:  40.618804931640625\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 197 (493717) @ Episode 265/10000, loss: 0.00053254852537065744\n",
      "Episode time:  29.643770933151245\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 200 (493917) @ Episode 266/10000, loss: 0.00010413017298560595\n",
      "Episode time:  29.95218276977539\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 245 (494162) @ Episode 267/10000, loss: 0.00024346671125385916\n",
      "Episode time:  37.35424613952637\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 287 (494449) @ Episode 268/10000, loss: 0.00010747000487754121\n",
      "Episode time:  44.36446404457092\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 230 (494679) @ Episode 269/10000, loss: 0.09948974102735526446\n",
      "Episode time:  35.657912731170654\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 230 (494909) @ Episode 270/10000, loss: 0.00191488675773143776\n",
      "Episode time:  35.28504014015198\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 187 (495096) @ Episode 271/10000, loss: 0.00039824284613132477\n",
      "Episode time:  29.01330018043518\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 218 (495314) @ Episode 272/10000, loss: 0.00032672236557118595\n",
      "Episode time:  32.96708703041077\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 202 (495516) @ Episode 273/10000, loss: 0.00534164858981967264\n",
      "Episode time:  30.475759029388428\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 274 (495790) @ Episode 274/10000, loss: 0.00430323835462331856\n",
      "Episode time:  41.26012325286865\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 334 (496124) @ Episode 275/10000, loss: 0.00482550263404846293\n",
      "Episode time:  50.77999711036682\n",
      "\n",
      "Episode Reward: 3.0\n",
      "Step 255 (496379) @ Episode 276/10000, loss: 0.00201959838159382348\n",
      "Episode time:  38.52607202529907\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 327 (496706) @ Episode 277/10000, loss: 0.00040441227611154328\n",
      "Episode time:  49.65379190444946\n",
      "\n",
      "Episode Reward: 3.0\n",
      "Step 254 (496960) @ Episode 278/10000, loss: 0.00764558883383870196\n",
      "Episode time:  38.69065713882446\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 188 (497148) @ Episode 279/10000, loss: 0.00034365794272162022\n",
      "Episode time:  28.502274990081787\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 203 (497351) @ Episode 280/10000, loss: 0.00032265664776787165\n",
      "Episode time:  30.28083300590515\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 315 (497666) @ Episode 281/10000, loss: 0.00486150523647666146\n",
      "Episode time:  47.72454285621643\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 321 (497987) @ Episode 282/10000, loss: 0.00018983319750986993\n",
      "Episode time:  48.80434584617615\n",
      "\n",
      "Episode Reward: 3.0\n",
      "Step 228 (498215) @ Episode 283/10000, loss: 0.00055361236445605755\n",
      "Episode time:  34.24175405502319\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 294 (498509) @ Episode 284/10000, loss: 0.00573470629751682316\n",
      "Episode time:  43.994441986083984\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 228 (498737) @ Episode 285/10000, loss: 0.00040488439844921233\n",
      "Episode time:  34.54682493209839\n",
      "\n",
      "Episode Reward: 1.0\n",
      "Step 395 (499132) @ Episode 286/10000, loss: 0.00027060636784881353\n",
      "Episode time:  59.13614892959595\n",
      "\n",
      "Episode Reward: 3.0\n",
      "Step 315 (499447) @ Episode 287/10000, loss: 0.00490130903199315173\n",
      "Episode time:  46.96537780761719\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 307 (499754) @ Episode 288/10000, loss: 0.02730997465550899553\n",
      "Episode time:  46.4236261844635\n",
      "\n",
      "Episode Reward: 2.0\n",
      "Step 191 (499945) @ Episode 289/10000, loss: 0.00488846004009246886\n",
      "Episode time:  28.517932891845703\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 55 (500000) @ Episode 290/10000, loss: 0.00015008370974101126"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 500000 is out of bounds for axis 0 with size 500000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-fd992ade7873>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m                                     \u001b[0mepsilon_decay_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                                     \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                                     batch_size=32):\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEpisode Reward: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-f645621673eb>\u001b[0m in \u001b[0;36mdeep_q_learning\u001b[0;34m(sess, env, q_estimator, target_estimator, state_processor, num_episodes, experiment_dir, replay_memory_size, replay_memory_init_size, update_target_estimator_every, discount_factor, epsilon_start, epsilon_end, epsilon_decay_steps, batch_size, record_video_every)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;31m# Take a step in the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon_decay_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0msuccessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 500000 is out of bounds for axis 0 with size 500000"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.abspath(\"./experiments/{}\".format(env.spec.id))\n",
    "\n",
    "# Create a global step variable\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "# Create estimators\n",
    "q_estimator = Estimator(scope=\"q\", summaries_dir=experiment_dir)\n",
    "target_estimator = Estimator(scope=\"target_q\")\n",
    "\n",
    "# State processor\n",
    "state_processor = StateProcessor()\n",
    "\n",
    "# Run it!\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for t, stats in deep_q_learning(sess,\n",
    "                                    env,\n",
    "                                    q_estimator=q_estimator,\n",
    "                                    target_estimator=target_estimator,\n",
    "                                    state_processor=state_processor,\n",
    "                                    experiment_dir=experiment_dir,\n",
    "                                    num_episodes=10000,\n",
    "                                    replay_memory_size=500000,\n",
    "                                    replay_memory_init_size=50000,\n",
    "                                    update_target_estimator_every=10000,\n",
    "                                    epsilon_start=1.0,\n",
    "                                    epsilon_end=0.1,\n",
    "                                    epsilon_decay_steps=500000,\n",
    "                                    discount_factor=0.99,\n",
    "                                    batch_size=32):\n",
    "\n",
    "        print(\"\\nEpisode Reward: {}\".format(stats.episode_rewards[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
