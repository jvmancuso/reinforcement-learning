{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pprint\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "from lib.envs.gridworld import GridworldEnv\n",
    "import gym\n",
    "from gym import wrappers\n",
    "from gym.envs.toy_text.frozen_lake import FrozenLakeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.0001, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment. env.P represents the transition probabilities of the environment.\n",
    "        theta: Stopping threshold. If the value of all states changes less than theta\n",
    "            in one iteration we are done.\n",
    "        discount_factor: lambda time discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V) of the optimal policy and the optimal value function.        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Value iteration\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.nS):\n",
    "            action_values = np.zeros(env.nA)\n",
    "            for a in range(env.nA):\n",
    "                for prob, successor, reward, done in env.P[s][a]:\n",
    "                    action_values[a] += prob*(reward + discount_factor*V[successor])\n",
    "            best_av = np.max(action_values)\n",
    "            delta = max(delta, np.abs(best_av-V[s]))\n",
    "            V[s] = best_av\n",
    "        if delta<theta:\n",
    "            break\n",
    "    \n",
    "    # Greedy policy\n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    for s in range(env.nS):\n",
    "        action_values = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            for prob, successor, reward, done in env.P[s][a]:\n",
    "                action_values[a] += prob*(reward + discount_factor*V[successor])\n",
    "        best_action = np.argmax(action_values)\n",
    "        policy[s,best_action] = 1\n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Probability Distribution:\n",
      "[[ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 1.  0.  0.  0.]]\n",
      "\n",
      "Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\n",
      "[[0 3 3 2]\n",
      " [0 0 0 2]\n",
      " [0 0 1 2]\n",
      " [0 1 1 0]]\n",
      "\n",
      "Value Function:\n",
      "[ 0. -1. -2. -3. -1. -2. -3. -2. -2. -3. -2. -1. -3. -2. -1.  0.]\n",
      "\n",
      "Reshaped Grid Value Function:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy, v = value_iteration(env)\n",
    "\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test the value function\n",
    "expected_v = np.array([ 0, -1, -2, -3, -1, -2, -3, -2, -2, -3, -2, -1, -3, -2, -1,  0])\n",
    "np.testing.assert_array_almost_equal(v, expected_v, decimal=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we train a new policy on the FrozenLake environment via value iteration.  FrozenLake is similar to Gridworld, except that certain cells in the grid are designated \"holes\". Falling into one of these holes automatically terminates the episode with reward 0.  Additionally, the  lake is slippery, so that actions are not completely deterministic. This means that taking an action of Right may actually move the agent in a direction other than Right with some slight probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-02 12:07:39,997] Making new env: FrozenLake-v0\n",
      "[2017-05-02 12:07:40,006] Clearing 22 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-02 12:07:40,010] Starting new video recorder writing to /Users/jasonmancuso/reinforcement-learning/DP/frozen-lake-experiment/openaigym.video.0.15582.video000000.json\n",
      "[2017-05-02 12:07:40,016] Starting new video recorder writing to /Users/jasonmancuso/reinforcement-learning/DP/frozen-lake-experiment/openaigym.video.0.15582.video000001.json\n",
      "[2017-05-02 12:07:40,021] Starting new video recorder writing to /Users/jasonmancuso/reinforcement-learning/DP/frozen-lake-experiment/openaigym.video.0.15582.video000002.json\n",
      "[2017-05-02 12:07:40,030] Starting new video recorder writing to /Users/jasonmancuso/reinforcement-learning/DP/frozen-lake-experiment/openaigym.video.0.15582.video000003.json\n",
      "[2017-05-02 12:07:40,034] Starting new video recorder writing to /Users/jasonmancuso/reinforcement-learning/DP/frozen-lake-experiment/openaigym.video.0.15582.video000004.json\n",
      "[2017-05-02 12:07:40,039] Starting new video recorder writing to /Users/jasonmancuso/reinforcement-learning/DP/frozen-lake-experiment/openaigym.video.0.15582.video000005.json\n",
      "[2017-05-02 12:07:40,043] Starting new video recorder writing to /Users/jasonmancuso/reinforcement-learning/DP/frozen-lake-experiment/openaigym.video.0.15582.video000006.json\n",
      "[2017-05-02 12:07:40,053] Starting new video recorder writing to /Users/jasonmancuso/reinforcement-learning/DP/frozen-lake-experiment/openaigym.video.0.15582.video000007.json\n",
      "[2017-05-02 12:07:40,063] Starting new video recorder writing to /Users/jasonmancuso/reinforcement-learning/DP/frozen-lake-experiment/openaigym.video.0.15582.video000008.json\n",
      "[2017-05-02 12:07:40,073] Starting new video recorder writing to /Users/jasonmancuso/reinforcement-learning/DP/frozen-lake-experiment/openaigym.video.0.15582.video000009.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation finished after 70 timesteps\n",
      "Episode reward was 1.0.\n",
      "Simulation finished after 21 timesteps\n",
      "Episode reward was 1.0.\n",
      "Simulation finished after 99 timesteps\n",
      "Episode reward was 0.0.\n",
      "Simulation finished after 29 timesteps\n",
      "Episode reward was 1.0.\n",
      "Simulation finished after 18 timesteps\n",
      "Episode reward was 1.0.\n",
      "Simulation finished after 15 timesteps\n",
      "Episode reward was 0.0.\n",
      "Simulation finished after 95 timesteps\n",
      "Episode reward was 1.0.\n",
      "Simulation finished after 99 timesteps\n",
      "Episode reward was 0.0.\n",
      "Simulation finished after 99 timesteps\n",
      "Episode reward was 0.0.\n",
      "Simulation finished after 43 timesteps\n",
      "Episode reward was 1.0.\n",
      "Average episode reward was 0.6.\n"
     ]
    }
   ],
   "source": [
    "env = FrozenLakeEnv()\n",
    "policy, v = value_iteration(env)\n",
    "\n",
    "max_time_steps = 100000\n",
    "n_episode = 1\n",
    "\n",
    "env = gym.make('FrozenLake-v0')\n",
    "env = wrappers.Monitor(env, './frozen-lake-experiment', video_callable=lambda x: True, force=True)\n",
    "\n",
    "average_reward = 0\n",
    "for i_episode in range(n_episode):\n",
    "\n",
    "    observation = env.reset() #reset environment to beginning \n",
    "    episode_reward = 0\n",
    "    #run for several time-steps\n",
    "    for t in range(max_time_steps): \n",
    "        #display experiment\n",
    "        #env.render() \n",
    "\n",
    "        #sample a random action \n",
    "        action = np.argmax(policy[observation])\n",
    "\n",
    "        #observe next step and get reward \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            #env.render() \n",
    "            print(\"Simulation finished after {0} timesteps\".format(t))\n",
    "            print(\"Episode reward was {0}.\".format(episode_reward))\n",
    "            break\n",
    "    average_reward += episode_reward\n",
    "\n",
    "average_reward /= n_episode\n",
    "print(\"Average episode reward was {0}.\".format(average_reward))\n",
    "env.render(close=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
